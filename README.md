# Explaining Safety Is Not Enforcing Safety

**Evans Tovar**  
Independent Researcher â€“ AI Safety & Governance  
ORCID: 0009-0008-4388-1836  
DOI: https://doi.org/10.17605/OSF.IO/AXBND  

---

## Overview

This repository hosts materials related to the preprint:

**Explaining Safety Is Not Enforcing Safety: Cross-Vendor Evidence of Contextual, Surface, and Epistemic Failures in Consumer AI Assistants** (2026).

The study reports longitudinal, cross-surface behavioral testing of four major consumer AI assistants and documents systematic *articulationâ€“application gaps*: cases where systems clearly explain safety rules and subsequently violate those same rules under realistic contextual drift.

Rather than evaluating safety by refusal quality in isolation, the work proposes assessing **constraint persistence across context** as a robustness metric.

---

## Core Contributions

- Cross-vendor behavioral comparison  
- Documentation of contextual drift as a safety erosion vector  
- Identification of surface-dependent constraint failure  
- Introduction of *epistemic honesty* and *epistemic friction* as safety primitives  
- Governance analysis of responsible disclosure handling  

---

## Access the Preprint

ðŸ“„ **Download PDF (OSF):**  
https://osf.io/axbnd/

ðŸ”— **DOI:**  
https://doi.org/10.17605/OSF.IO/AXBND  

---

## Repository Scope

This repository may include:

- Supplementary methodological notes  
- Redacted example transcripts (where permissible)  
- Disclosure timelines  
- Version updates  

---

## Citation

If you wish to cite this work:

Tovar, E. (2026). *Explaining Safety Is Not Enforcing Safety: Cross-Vendor Evidence of Contextual, Surface, and Epistemic Failures in Consumer AI Assistants*. OSF Preprint. https://doi.org/10.17605/OSF.IO/AXBND  

---

## Contact

For questions, corrections, or collaboration inquiries:  
ai.safety.eftovar@gmail.com
